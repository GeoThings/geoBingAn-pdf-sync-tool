#!/usr/bin/env python3
"""
ä¸Šå‚³æœ€æ–° 5 ç­† PDF åˆ° geoBingAn åˆ†æå·¥å…·ï¼ˆæ•ˆèƒ½å„ªåŒ–ç‰ˆæœ¬ï¼‰

åŠŸèƒ½ï¼š
1. æƒæ Google Drive Shared Drive ä¸­çš„å»ºæ¡ˆ PDF
2. åªä¸Šå‚³æœ€æ–°çš„ 5 å€‹ PDF æª”æ¡ˆ
3. è¨˜éŒ„å·²ä¸Šå‚³çš„ PDFï¼Œé¿å…é‡è¤‡è™•ç†
4. ä½¿ç”¨ JWT èªè­‰ï¼ˆjerryjo0802@gmail.comï¼‰
5. ã€æ–°å¢ã€‘ä¸¦è¡Œä¸Šå‚³æ”¯æ´ï¼ˆå¯é¸ï¼‰
6. ã€æ–°å¢ã€‘JWT Token è‡ªå‹•åˆ·æ–°
"""
import json
import os
import sys
import io
import time
import base64
import requests
from pathlib import Path
from datetime import datetime, timedelta
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaIoBaseDownload
from typing import Dict, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# åŒ¯å…¥é…ç½®æª”æ¡ˆ
try:
    from config import (
        JWT_TOKEN,
        USER_EMAIL,
        GROUP_ID,
        GEOBINGAN_API_URL,
        REFRESH_TOKEN,
        GEOBINGAN_REFRESH_URL
    )
    print(f"âœ… å·²è¼‰å…¥èªè­‰é…ç½®ï¼ˆç”¨æˆ¶: {USER_EMAIL}ï¼‰")
except ImportError as e:
    print("âŒ æ‰¾ä¸åˆ° config.py æˆ–ç¼ºå°‘å¿…è¦è¨­å®š")
    print(f"   éŒ¯èª¤: {e}")
    print("   è«‹åƒè€ƒ config.py.example å»ºç«‹ config.py")
    sys.exit(1)

# å…¨åŸŸè®Šæ•¸ï¼šç•¶å‰ä½¿ç”¨çš„ Token
current_access_token = JWT_TOKEN
token_lock = threading.Lock()

# ================== è¨­å®šå€åŸŸ ==================
# Google Drive èªè­‰
SERVICE_ACCOUNT_FILE = '/Users/geothingsmacbookair/Downloads/credentials.json'
SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
SHARED_DRIVE_ID = '0AIvp1h-6BZ1oUk9PVA'

# geoBingAn API è¨­å®šï¼ˆå¾ config.py åŒ¯å…¥ï¼‰
# GEOBINGAN_API_URL - å·²å¾ config.py åŒ¯å…¥
# ä½¿ç”¨ construction-reports/upload/ ç«¯é»ï¼Œèˆ‡ç¶²é ä¸Šå‚³ç›¸åŒ

# ç‹€æ…‹è¿½è¹¤æª”æ¡ˆ
STATE_FILE = './state/uploaded_to_geobingan_7days.json'
HISTORY_FILE = './state/upload_history_all.json'  # æ°¸ä¹…æ­·å²è¨˜éŒ„

# æ—¥æœŸéæ¿¾è¨­å®š
DAYS_AGO = 7  # ä¸Šå‚³æœ€è¿‘ 7 å¤©æ›´æ–°çš„ PDF

# æ‰¹æ¬¡ä¸Šå‚³è¨­å®š
MAX_UPLOADS = 100  # æ¯æ¬¡ä¸Šå‚³æœ€æ–° 100 ç­† PDF

# é€Ÿç‡æ§åˆ¶ï¼šæ¯æ¬¡ä¸Šå‚³ä¹‹é–“çš„å»¶é²ï¼ˆç§’ï¼‰
DELAY_BETWEEN_UPLOADS = 2  # åŠ é€Ÿï¼šæ¸›å°‘åˆ° 2 ç§’

# ä¸¦è¡Œä¸Šå‚³è¨­å®š
ENABLE_PARALLEL_UPLOAD = False  # è¨­ç‚º True å•Ÿç”¨ä¸¦è¡Œä¸Šå‚³ï¼ˆå¯¦é©—æ€§åŠŸèƒ½ï¼‰
MAX_WORKERS = 3  # ä¸¦è¡Œä¸Šå‚³çš„æœ€å¤§åŸ·è¡Œç·’æ•¸

# è‡ªå‹•ç¢ºèªï¼ˆæ¸¬è©¦æ¨¡å¼ï¼‰
AUTO_CONFIRM = True  # å•Ÿç”¨è‡ªå‹•ç¢ºèªé€²è¡Œæ‰¹æ¬¡ä¸Šå‚³

# æ’é™¤æ¸…å–®ï¼šä¸ä¸Šå‚³çš„æª”æ¡ˆï¼ˆç¯„ä¾‹æª”ã€æ¸¬è©¦æª”ç­‰ï¼‰
EXCLUDE_FILES = [
    'é›²ç«¯è³‡æ–™åº«è¨­ç½®ä¹‹ç¯„ä¾‹.pdf',  # ç¯„ä¾‹æª”æ¡ˆï¼Œå…§å«å‡è³‡æ–™
    'é›²ç«¯è³‡æ–™åº«è¨­ç½®ä¹‹ç¯„ä¾‹',       # ç„¡å‰¯æª”åç‰ˆæœ¬
]
# ============================================

# å…¨åŸŸé–ï¼Œç”¨æ–¼ä¸¦è¡Œä¸Šå‚³æ™‚ä¿è­·ç‹€æ…‹æª”æ¡ˆ
state_lock = threading.Lock()


# ================== JWT Token ç®¡ç† ==================

def decode_jwt_payload(token: str) -> dict:
    """è§£ç¢¼ JWT Token çš„ payloadï¼ˆä¸é©—è­‰ç°½åï¼‰"""
    try:
        # JWT æ ¼å¼: header.payload.signature
        parts = token.split('.')
        if len(parts) != 3:
            return {}

        # Base64 è§£ç¢¼ payloadï¼ˆéœ€è¦è™•ç† paddingï¼‰
        payload = parts[1]
        # æ·»åŠ  padding
        padding = 4 - len(payload) % 4
        if padding != 4:
            payload += '=' * padding

        decoded = base64.urlsafe_b64decode(payload)
        return json.loads(decoded)
    except Exception:
        return {}


def is_token_expired(token: str, buffer_seconds: int = 300) -> bool:
    """
    æª¢æŸ¥ Token æ˜¯å¦å·²éæœŸæˆ–å³å°‡éæœŸ

    Args:
        token: JWT Token
        buffer_seconds: æå‰å¤šå°‘ç§’è¦–ç‚ºéæœŸï¼ˆé è¨­ 5 åˆ†é˜ï¼‰

    Returns:
        True å¦‚æœå·²éæœŸæˆ–å³å°‡éæœŸ
    """
    payload = decode_jwt_payload(token)
    if not payload:
        return True

    exp = payload.get('exp')
    if not exp:
        return True

    # æª¢æŸ¥æ˜¯å¦éæœŸï¼ˆåŠ ä¸Šç·©è¡æ™‚é–“ï¼‰
    current_time = time.time()
    return current_time >= (exp - buffer_seconds)


def refresh_access_token() -> Optional[str]:
    """
    ä½¿ç”¨ refresh_token å–å¾—æ–°çš„ access_token

    Returns:
        æ–°çš„ access_tokenï¼Œå¤±æ•—æ™‚è¿”å› None
    """
    global current_access_token

    try:
        print("ğŸ”„ æ­£åœ¨åˆ·æ–° JWT Token...")

        response = requests.post(
            GEOBINGAN_REFRESH_URL,
            json={'refresh_token': REFRESH_TOKEN},
            headers={'Content-Type': 'application/json'},
            timeout=30
        )

        if response.status_code == 200:
            data = response.json()
            new_token = data.get('access') or data.get('access_token')

            if new_token:
                with token_lock:
                    current_access_token = new_token

                # æ›´æ–° config.py ä¸­çš„ token
                update_config_token(new_token)

                print("âœ… JWT Token åˆ·æ–°æˆåŠŸ")
                return new_token
            else:
                print(f"âŒ åˆ·æ–°å›æ‡‰ä¸­æ‰¾ä¸åˆ° access token: {data}")
                return None
        else:
            print(f"âŒ Token åˆ·æ–°å¤±æ•— ({response.status_code}): {response.text[:200]}")
            return None

    except Exception as e:
        print(f"âŒ Token åˆ·æ–°ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None


def update_config_token(new_token: str):
    """
    æ›´æ–° config.py ä¸­çš„ JWT_TOKEN
    """
    config_path = Path(__file__).parent / 'config.py'

    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æ›¿æ› token
        import re
        pattern = r"JWT_TOKEN = '[^']+'"
        replacement = f"JWT_TOKEN = '{new_token}'"
        new_content = re.sub(pattern, replacement, content)

        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(new_content)

        print(f"ğŸ“ å·²æ›´æ–° config.py ä¸­çš„ Token")
    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•æ›´æ–° config.py: {e}")


def get_valid_token() -> str:
    """
    å–å¾—æœ‰æ•ˆçš„ access token

    å¦‚æœç•¶å‰ token å³å°‡éæœŸï¼Œæœƒè‡ªå‹•åˆ·æ–°

    Returns:
        æœ‰æ•ˆçš„ access token
    """
    global current_access_token

    with token_lock:
        if is_token_expired(current_access_token):
            print("âš ï¸  JWT Token å·²éæœŸæˆ–å³å°‡éæœŸ")
            new_token = refresh_access_token()
            if new_token:
                return new_token
            else:
                print("âš ï¸  ä½¿ç”¨èˆŠ Token å˜—è©¦ï¼ˆå¯èƒ½æœƒå¤±æ•—ï¼‰")

        return current_access_token


# ================== ç‹€æ…‹ç®¡ç† ==================

def load_history() -> dict:
    """è¼‰å…¥æ°¸ä¹…ä¸Šå‚³æ­·å²è¨˜éŒ„"""
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        'uploaded_files': [],
        'total_count': 0,
        'first_upload': None,
        'last_upload': None
    }


def save_history(history: dict):
    """å„²å­˜æ°¸ä¹…ä¸Šå‚³æ­·å²è¨˜éŒ„"""
    os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, indent=2, ensure_ascii=False, fp=f)


def add_to_history(filepath: str):
    """æ–°å¢æª”æ¡ˆåˆ°æ°¸ä¹…æ­·å²è¨˜éŒ„"""
    history = load_history()

    if filepath not in history['uploaded_files']:
        history['uploaded_files'].append(filepath)
        history['total_count'] = len(history['uploaded_files'])
        history['last_upload'] = datetime.now().isoformat()

        if not history['first_upload']:
            history['first_upload'] = history['last_upload']

        save_history(history)


def load_state() -> dict:
    """è¼‰å…¥å·²ä¸Šå‚³çš„ PDF è¨˜éŒ„ï¼ˆåŒ…å«å¿«å–ï¼‰"""
    if os.path.exists(STATE_FILE):
        with open(STATE_FILE, 'r', encoding='utf-8') as f:
            state = json.load(f)
            # ç¢ºä¿æœ‰å¿«å–çµæ§‹
            if 'cache' not in state:
                state['cache'] = {
                    'folders': [],
                    'pdfs': [],
                    'last_scan': None
                }
            return state
    return {
        'uploaded_files': [],
        'errors': [],
        'cache': {
            'folders': [],
            'pdfs': [],
            'last_scan': None
        }
    }


def save_state(state: dict):
    """å„²å­˜å·²ä¸Šå‚³çš„ PDF è¨˜éŒ„

    æ³¨æ„ï¼šæ­¤å‡½æ•¸ä¸åŒ…å«é–ï¼Œå‘¼å«è€…éœ€è¦è‡ªè¡Œç®¡ç† state_lock
    """
    os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)
    with open(STATE_FILE, 'w', encoding='utf-8') as f:
        json.dump(state, indent=2, ensure_ascii=False, fp=f)


def get_drive_service():
    """åˆå§‹åŒ– Google Drive APIï¼ˆService Accountï¼‰"""
    print("ğŸ”‘ åˆå§‹åŒ– Google Drive API (Service Account)")

    if not os.path.exists(SERVICE_ACCOUNT_FILE):
        print(f"âŒ æ‰¾ä¸åˆ° Service Account é‡‘é‘°: {SERVICE_ACCOUNT_FILE}")
        sys.exit(1)

    credentials = service_account.Credentials.from_service_account_file(
        SERVICE_ACCOUNT_FILE, scopes=SCOPES)

    service = build('drive', 'v3', credentials=credentials)
    print(f"âœ… å·²åˆå§‹åŒ– ({credentials.service_account_email})")
    return service


def list_project_folders(service, use_cache: bool = True, state: dict = None, days_ago: int = 7) -> List[Dict]:
    """
    åˆ—å‡ºå»ºæ¡ˆè³‡æ–™å¤¾ï¼ˆæ”¯æ´å¿«å–å’Œæ™ºæ…§æƒæï¼‰

    Args:
        service: Google Drive API service
        use_cache: æ˜¯å¦ä½¿ç”¨å¿«å–
        state: ç‹€æ…‹æª”æ¡ˆï¼ˆåŒ…å«å¿«å–ï¼‰
        days_ago: åªæƒææœ€è¿‘ N å¤©ä¿®æ”¹çš„è³‡æ–™å¤¾ï¼ˆæ™ºæ…§æƒæï¼‰
    """
    # æª¢æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨å¿«å–
    if use_cache and state and state.get('cache', {}).get('last_scan'):
        last_scan = state['cache']['last_scan']
        last_scan_time = datetime.fromisoformat(last_scan.replace('Z', '+00:00'))
        now = datetime.now(last_scan_time.tzinfo)

        # å¦‚æœä¸Šæ¬¡æƒæåœ¨ 24 å°æ™‚å…§ï¼Œä½¿ç”¨å¿«å–
        if (now - last_scan_time).total_seconds() < 86400:  # 24 hours
            cached_folders = state['cache'].get('folders', [])
            if cached_folders:
                print(f"âœ… ä½¿ç”¨å¿«å–çš„è³‡æ–™å¤¾åˆ—è¡¨ï¼ˆ{len(cached_folders)} å€‹ï¼Œä¸Šæ¬¡æƒæ: {last_scan}ï¼‰")
                return cached_folders

    try:
        # æ™ºæ…§æƒæï¼šåªæƒææœ€è¿‘ N å¤©ä¿®æ”¹çš„è³‡æ–™å¤¾
        cutoff_date = datetime.now() - timedelta(days=days_ago)
        cutoff_date_str = cutoff_date.isoformat() + 'Z'

        print(f"ğŸ” æ™ºæ…§æƒæ: åªåˆ—å‡ºæœ€è¿‘ {days_ago} å¤©ä¿®æ”¹çš„è³‡æ–™å¤¾...")

        query = (
            f"modifiedTime >= '{cutoff_date_str}' and "
            "mimeType = 'application/vnd.google-apps.folder' and "
            "trashed = false"
        )

        results = service.files().list(
            q=query,
            corpora='drive',
            driveId=SHARED_DRIVE_ID,
            includeItemsFromAllDrives=True,
            supportsAllDrives=True,
            pageSize=1000,
            fields='files(id, name, modifiedTime)'
        ).execute()

        folders = results.get('files', [])

        # æ›´æ–°å¿«å–
        if state is not None:
            state['cache']['folders'] = folders
            state['cache']['last_scan'] = datetime.now().isoformat() + 'Z'

        print(f"âœ… æ‰¾åˆ° {len(folders)} å€‹æœ€è¿‘ {days_ago} å¤©ä¿®æ”¹çš„è³‡æ–™å¤¾")
        return folders

    except HttpError as e:
        print(f"âŒ åˆ—å‡ºè³‡æ–™å¤¾å¤±æ•—: {e}")
        return []


def list_all_pdfs_with_folder_info(service, folders: List[Dict], use_cache: bool = True, state: dict = None) -> List[Dict]:
    """
    åˆ—å‡ºæ‰€æœ‰è³‡æ–™å¤¾ä¸­çš„ PDFï¼Œä¸¦é™„åŠ è³‡æ–™å¤¾è³‡è¨Šï¼ˆæ”¯æ´å¿«å–ï¼‰

    Returns:
        List of dict with keys: id, name, size, modifiedTime, folder_id, folder_name
    """
    # æª¢æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨å¿«å–
    if use_cache and state and state.get('cache', {}).get('last_scan'):
        last_scan = state['cache']['last_scan']
        last_scan_time = datetime.fromisoformat(last_scan.replace('Z', '+00:00'))
        now = datetime.now(last_scan_time.tzinfo)

        # å¦‚æœä¸Šæ¬¡æƒæåœ¨ 24 å°æ™‚å…§ï¼Œä½¿ç”¨å¿«å–
        if (now - last_scan_time).total_seconds() < 86400:  # 24 hours
            cached_pdfs = state['cache'].get('pdfs', [])
            if cached_pdfs:
                print(f"âœ… ä½¿ç”¨å¿«å–çš„ PDF åˆ—è¡¨ï¼ˆ{len(cached_pdfs)} å€‹ï¼‰")
                return cached_pdfs

    print(f"ğŸ” æƒæ {len(folders)} å€‹è³‡æ–™å¤¾ä¸­çš„ PDF...")
    all_pdfs = []

    for idx, folder in enumerate(folders, 1):
        folder_id = folder['id']
        folder_name = folder['name']

        if idx % 10 == 0:
            print(f"  é€²åº¦: {idx}/{len(folders)} å€‹è³‡æ–™å¤¾...")

        try:
            query = (
                f"'{folder_id}' in parents and "
                f"mimeType = 'application/pdf' and "
                f"trashed = false"
            )

            results = service.files().list(
                q=query,
                pageSize=1000,
                includeItemsFromAllDrives=True,
                supportsAllDrives=True,
                fields='files(id, name, size, modifiedTime)'
            ).execute()

            pdfs = results.get('files', [])

            # ç‚ºæ¯å€‹ PDF æ·»åŠ è³‡æ–™å¤¾è³‡è¨Š
            for pdf in pdfs:
                pdf['folder_id'] = folder_id
                pdf['folder_name'] = folder_name
                all_pdfs.append(pdf)

        except HttpError as e:
            print(f"  âŒ åˆ—å‡º {folder_name} çš„ PDF å¤±æ•—: {e}")
            continue

    # æ›´æ–°å¿«å–
    if state is not None:
        state['cache']['pdfs'] = all_pdfs

    return all_pdfs


def download_pdf(service, file_id: str, file_name: str, max_retries: int = 3) -> Optional[bytes]:
    """å¾ Google Drive ä¸‹è¼‰ PDFï¼ˆæ”¯æ´ç¶²è·¯éŒ¯èª¤é‡è©¦ï¼‰"""
    for attempt in range(max_retries):
        try:
            request = service.files().get_media(fileId=file_id)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, request)

            done = False
            while not done:
                status, done = downloader.next_chunk()

            fh.seek(0)
            content = fh.read()
            print(f"  ğŸ“¥ å·²ä¸‹è¼‰: {file_name} ({len(content)} bytes)")
            return content

        except (ConnectionError, TimeoutError, OSError) as e:
            # ç¶²è·¯éŒ¯èª¤ï¼šConnection reset, Timeout ç­‰
            if attempt < max_retries - 1:
                wait_time = 5 * (attempt + 1)  # æ¼¸é€²ç­‰å¾…ï¼š5s, 10s, 15s
                print(f"  âš ï¸  ç¶²è·¯éŒ¯èª¤ ({type(e).__name__}): {e}")
                print(f"  ğŸ”„ ç¬¬ {attempt + 1}/{max_retries} æ¬¡é‡è©¦ï¼Œç­‰å¾… {wait_time} ç§’...")
                time.sleep(wait_time)
            else:
                print(f"  âŒ ä¸‹è¼‰å¤±æ•—ï¼ˆå·²é‡è©¦ {max_retries} æ¬¡ï¼‰: {file_name} - {e}")
                return None

        except HttpError as e:
            print(f"  âŒ ä¸‹è¼‰å¤±æ•—: {file_name} - {e}")
            return None

        except Exception as e:
            print(f"  âŒ æœªé æœŸéŒ¯èª¤: {file_name} - {type(e).__name__}: {e}")
            return None

    return None


def upload_to_geobingan(pdf_content: bytes, file_name: str, project_code: str) -> Optional[dict]:
    """
    ä¸Šå‚³ PDF åˆ° geoBingAn API é€²è¡Œåˆ†æ

    ä½¿ç”¨ construction-reports/upload/ ç«¯é»ï¼ˆèˆ‡ç¶²é ä¸Šå‚³ç›¸åŒï¼‰
    ä½¿ç”¨ JWT Bearer Token èªè­‰

    æ³¨æ„ï¼š504 Gateway Timeout ä¸ä»£è¡¨å¤±æ•—ï¼Œå¾Œç«¯å¯èƒ½ä»åœ¨è™•ç†ä¸­
    """
    try:
        # ç¢ºä¿æª”åæœ‰ .pdf å‰¯æª”åï¼ˆæŸäº› Google Drive æª”æ¡ˆæ²’æœ‰å‰¯æª”åï¼‰
        if not file_name.lower().endswith('.pdf'):
            file_name = file_name + '.pdf'
            print(f"  â„¹ï¸  è‡ªå‹•åŠ ä¸Š .pdf å‰¯æª”å: {file_name}")

        files = {
            'file': (file_name, pdf_content, 'application/pdf')
        }

        # ä½¿ç”¨ construction-reports ç«¯é»çš„åƒæ•¸æ ¼å¼
        data = {
            'group_id': GROUP_ID,
            'report_type': 'weekly',  # daily, weekly, monthly, incident, inspection
            'primary_language': 'zh-TW'
        }

        # è¨­å®š JWT èªè­‰æ¨™é ­ï¼ˆä½¿ç”¨è‡ªå‹•åˆ·æ–°çš„ Tokenï¼‰
        valid_token = get_valid_token()
        headers = {
            'Authorization': f'Bearer {valid_token}'
        }

        response = requests.post(
            GEOBINGAN_API_URL,
            files=files,
            data=data,
            headers=headers,
            timeout=600  # 10 åˆ†é˜ï¼Œçµ¦å¾Œç«¯è¶³å¤ æ™‚é–“è™•ç† AI åˆ†æ
        )

        if response.status_code in [200, 201, 202]:
            result = response.json()
            report_id = result.get('id') or result.get('report_id', 'N/A')
            parse_status = result.get('parse_status', '')

            print(f"  âœ… ä¸Šå‚³æˆåŠŸï¼")
            print(f"     - Report ID: {report_id}")
            if parse_status:
                print(f"     - è§£æç‹€æ…‹: {parse_status}")
            if result.get('message'):
                print(f"     - {result.get('message')}")
            return result
        elif response.status_code == 504:
            # 504 Gateway Timeout - å¾Œç«¯å¯èƒ½ä»åœ¨è™•ç†ä¸­
            print(f"  â³ å·²é€å‡ºï¼Œå¾Œç«¯è™•ç†ä¸­ï¼ˆ504 Timeoutï¼Œé€™æ˜¯æ­£å¸¸çš„ï¼‰")
            print(f"     PDF å·²æˆåŠŸå‚³é€åˆ°ä¼ºæœå™¨ï¼ŒAI åˆ†æéœ€è¦ 2-5 åˆ†é˜")
            print(f"     å¾Œç«¯æœƒåœ¨èƒŒæ™¯å®Œæˆè™•ç†ï¼Œç¨å¾Œå¯åœ¨ç³»çµ±ä¸­æŸ¥çœ‹çµæœ")
            return {
                'status': 'processing',
                'message': 'PDF uploaded, backend processing in background',
                'file_name': file_name,
                'project_code': project_code
            }
        elif response.status_code == 502:
            # 502 Bad Gateway - é¡ä¼¼ 504ï¼Œå¾Œç«¯å¯èƒ½ä»åœ¨è™•ç†
            print(f"  â³ å·²é€å‡ºï¼Œå¾Œç«¯è™•ç†ä¸­ï¼ˆ502 Gatewayï¼Œé€™æ˜¯æ­£å¸¸çš„ï¼‰")
            return {
                'status': 'processing',
                'message': 'PDF uploaded, backend processing in background',
                'file_name': file_name,
                'project_code': project_code
            }
        elif response.status_code == 401:
            # Token éæœŸï¼Œå˜—è©¦åˆ·æ–°ä¸¦é‡è©¦
            print(f"  âš ï¸  Token å·²éæœŸï¼Œå˜—è©¦åˆ·æ–°...")
            new_token = refresh_access_token()
            if new_token:
                # ä½¿ç”¨æ–° Token é‡è©¦ä¸€æ¬¡
                headers['Authorization'] = f'Bearer {new_token}'
                retry_response = requests.post(
                    GEOBINGAN_API_URL,
                    files={'file': (file_name, pdf_content, 'application/pdf')},
                    data=data,
                    headers=headers,
                    timeout=600
                )
                if retry_response.status_code in [200, 201, 202]:
                    result = retry_response.json()
                    report_id = result.get('id') or result.get('report_id', 'N/A')
                    print(f"  âœ… é‡è©¦æˆåŠŸï¼Report ID: {report_id}")
                    return result
                else:
                    print(f"  âŒ é‡è©¦å¤±æ•— ({retry_response.status_code})")
                    return None
            else:
                print(f"  âŒ Token åˆ·æ–°å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒä¸Šå‚³")
                return None
        else:
            print(f"  âŒ API éŒ¯èª¤ ({response.status_code}): {response.text[:300]}")
            return None

    except requests.exceptions.Timeout:
        # Client timeout - ä½† PDF å¯èƒ½å·²ç¶“åˆ°é”ä¼ºæœå™¨
        print(f"  â³ é€£ç·šè¶…æ™‚ï¼Œä½† PDF å¯èƒ½å·²é€é”ä¼ºæœå™¨")
        print(f"     å¾Œç«¯ AI åˆ†æéœ€è¦è¼ƒé•·æ™‚é–“ï¼Œè«‹ç¨å¾Œåœ¨ç³»çµ±ä¸­ç¢ºèª")
        return {
            'status': 'processing',
            'message': 'Connection timeout, but PDF may have been received',
            'file_name': file_name,
            'project_code': project_code
        }
    except Exception as e:
        print(f"  âŒ ä¸Šå‚³å¤±æ•—: {e}")
        return None


def process_single_pdf(service, pdf: Dict, state: dict, idx: int, total: int) -> Dict:
    """
    è™•ç†å–®å€‹ PDF çš„ä¸‹è¼‰å’Œä¸Šå‚³ï¼ˆå¯ç”¨æ–¼ä¸¦è¡Œè™•ç†ï¼‰

    Returns:
        Dict with keys: success (bool), pdf (Dict), result (Optional[dict])
    """
    print(f"\n[{idx}/{total}] è™•ç†: {pdf['folder_name']}/{pdf['name']}")

    # ä¸‹è¼‰
    pdf_content = download_pdf(service, pdf['id'], pdf['name'])
    if not pdf_content:
        return {'success': False, 'pdf': pdf, 'result': None, 'error': 'download_failed'}

    # ä¸Šå‚³
    result = upload_to_geobingan(pdf_content, pdf['name'], pdf['folder_name'])

    if result:
        unique_id = f"{pdf['folder_name']}/{pdf['name']}"
        with state_lock:
            state['uploaded_files'].append(unique_id)
            save_state(state)
        # åŒæ™‚å„²å­˜åˆ°æ°¸ä¹…æ­·å²è¨˜éŒ„
        add_to_history(unique_id)
        return {'success': True, 'pdf': pdf, 'result': result}
    else:
        with state_lock:
            state['errors'].append({
                'folder': pdf['folder_name'],
                'file': pdf['name'],
                'file_id': pdf['id']
            })
            save_state(state)
        return {'success': False, 'pdf': pdf, 'result': None, 'error': 'upload_failed'}


def main():
    """ä¸»ç¨‹å¼"""
    print("\n" + "=" * 60)
    print("ğŸš€ ä¸Šå‚³æœ€æ–° 5 ç­† PDF åˆ° geoBingAnï¼ˆæ•ˆèƒ½å„ªåŒ–ç‰ˆï¼‰")
    if ENABLE_PARALLEL_UPLOAD:
        print(f"   âš¡ ä¸¦è¡Œä¸Šå‚³æ¨¡å¼ï¼ˆ{MAX_WORKERS} åŸ·è¡Œç·’ï¼‰")
    print("=" * 60)

    # åˆå§‹åŒ–
    service = get_drive_service()
    state = load_state()

    print(f"\nğŸ“Š ç‹€æ…‹:")
    print(f"  å·²ä¸Šå‚³: {len(state['uploaded_files'])} å€‹æª”æ¡ˆ")
    print(f"  éŒ¯èª¤è¨˜éŒ„: {len(state['errors'])} ç­†")

    # åˆ—å‡ºå»ºæ¡ˆè³‡æ–™å¤¾ï¼ˆä½¿ç”¨æ™ºæ…§æƒæå’Œå¿«å–ï¼‰
    print(f"\nğŸ“ åˆ—å‡ºå»ºæ¡ˆè³‡æ–™å¤¾ï¼ˆæ™ºæ…§æƒææ¨¡å¼ï¼‰...")
    project_folders = list_project_folders(service, use_cache=True, state=state, days_ago=DAYS_AGO)

    if not project_folders:
        print("âš ï¸  æœªæ‰¾åˆ°æœ€è¿‘ä¿®æ”¹çš„è³‡æ–™å¤¾ï¼Œå˜—è©¦å®Œæ•´æƒæ...")
        # å›é€€ï¼šå®Œæ•´æƒæï¼ˆä¸ä½¿ç”¨æ™‚é–“éæ¿¾ï¼‰
        try:
            query = "mimeType = 'application/vnd.google-apps.folder' and trashed = false"
            results = service.files().list(
                q=query,
                corpora='drive',
                driveId=SHARED_DRIVE_ID,
                includeItemsFromAllDrives=True,
                supportsAllDrives=True,
                pageSize=1000,
                fields='files(id, name, modifiedTime)'
            ).execute()
            project_folders = results.get('files', [])
            print(f"âœ… å®Œæ•´æƒææ‰¾åˆ° {len(project_folders)} å€‹è³‡æ–™å¤¾")
        except Exception as e:
            print(f"âŒ å®Œæ•´æƒæå¤±æ•—: {e}")
            sys.exit(0)

    # æ”¶é›† PDFï¼ˆä½¿ç”¨å¿«å–ï¼‰
    print(f"\nğŸ“„ æ”¶é›† PDF æª”æ¡ˆ...")
    all_pdfs = list_all_pdfs_with_folder_info(service, project_folders, use_cache=True, state=state)

    if not all_pdfs:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• PDF æª”æ¡ˆ")
        sys.exit(0)

    # å„²å­˜å¿«å–ï¼ˆåœ¨é–‹å§‹éæ¿¾ä¹‹å‰ï¼‰
    with state_lock:
        save_state(state)

    print(f"âœ… æ‰¾åˆ° {len(all_pdfs)} å€‹ PDF æª”æ¡ˆ")

    # è¨ˆç®—æ—¥æœŸé–¾å€¼ï¼ˆæœ€è¿‘ N å¤©ï¼‰
    cutoff_date = datetime.now() - timedelta(days=DAYS_AGO)
    cutoff_date_str = cutoff_date.isoformat() + 'Z'

    print(f"\nğŸ—“ï¸  éæ¿¾æœ€è¿‘ {DAYS_AGO} å¤©æ›´æ–°çš„ PDFï¼ˆ{cutoff_date.strftime('%Y-%m-%d')} ä¹‹å¾Œï¼‰...")

    # éæ¿¾æœ€è¿‘ N å¤©çš„ PDF
    recent_pdfs = []
    for pdf in all_pdfs:
        # Google Drive çš„ modifiedTime æ ¼å¼: '2025-12-29T06:11:04.237Z'
        if pdf['modifiedTime'] >= cutoff_date_str:
            recent_pdfs.append(pdf)

    print(f"âœ… æ‰¾åˆ° {len(recent_pdfs)} å€‹æœ€è¿‘ {DAYS_AGO} å¤©æ›´æ–°çš„ PDF")

    # æ’åºï¼šæŒ‰ä¿®æ”¹æ™‚é–“é™åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
    recent_pdfs.sort(key=lambda x: x['modifiedTime'], reverse=True)

    # éæ¿¾æ‰å·²ä¸Šå‚³çš„å’Œæ’é™¤æ¸…å–®ä¸­çš„æª”æ¡ˆï¼Œæœ€å¤šå– MAX_UPLOADS ç­†
    pdfs_to_upload = []
    excluded_count = 0
    for pdf in recent_pdfs:
        # æª¢æŸ¥æ˜¯å¦åœ¨æ’é™¤æ¸…å–®ä¸­
        if pdf['name'] in EXCLUDE_FILES:
            excluded_count += 1
            continue

        unique_id = f"{pdf['folder_name']}/{pdf['name']}"
        if unique_id not in state['uploaded_files']:
            pdfs_to_upload.append(pdf)
            if len(pdfs_to_upload) >= MAX_UPLOADS:
                break

    if excluded_count > 0:
        print(f"â­ï¸  å·²è·³é {excluded_count} å€‹æ’é™¤æ¸…å–®ä¸­çš„æª”æ¡ˆ")

    if not pdfs_to_upload:
        print(f"\nâš ï¸  æœ€æ–°çš„ {MAX_UPLOADS} ç­† PDF éƒ½å·²ä¸Šå‚³éäº†ï¼")
        print("\nå¦‚è¦é‡æ–°ä¸Šå‚³ï¼Œè«‹åˆªé™¤ç‹€æ…‹æª”æ¡ˆ:")
        print(f"  rm {STATE_FILE}")
        sys.exit(0)

    print(f"\nğŸ“‹ å°‡ä¸Šå‚³ä»¥ä¸‹ {len(pdfs_to_upload)} å€‹æœ€æ–° PDF:")
    for i, pdf in enumerate(pdfs_to_upload, 1):
        print(f"{i}. {pdf['folder_name']}/{pdf['name']}")
        print(f"   ä¿®æ”¹æ™‚é–“: {pdf['modifiedTime']}")

    # è©¢å•ç¢ºèª
    if AUTO_CONFIRM:
        print(f"\nğŸ¤– è‡ªå‹•ç¢ºèªæ¨¡å¼ï¼šå°‡ä¸Šå‚³é€™ {len(pdfs_to_upload)} å€‹æª”æ¡ˆ")
    else:
        response = input(f"\næ˜¯å¦ç¹¼çºŒä¸Šå‚³é€™ {len(pdfs_to_upload)} å€‹æª”æ¡ˆ? (y/n): ")
        if response.lower() != 'y':
            print("ğŸ‘‹ å·²å–æ¶ˆ")
            sys.exit(0)

    # ä¸Šå‚³
    print("\n" + "=" * 60)
    print("é–‹å§‹ä¸Šå‚³")
    print("=" * 60)

    success_count = 0
    error_count = 0

    if ENABLE_PARALLEL_UPLOAD:
        # ä¸¦è¡Œä¸Šå‚³æ¨¡å¼
        print(f"âš¡ ä½¿ç”¨ä¸¦è¡Œä¸Šå‚³ï¼ˆ{MAX_WORKERS} åŸ·è¡Œç·’ï¼‰")

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_pdf = {
                executor.submit(process_single_pdf, service, pdf, state, idx, len(pdfs_to_upload)): pdf
                for idx, pdf in enumerate(pdfs_to_upload, 1)
            }

            # è™•ç†å®Œæˆçš„ä»»å‹™
            for future in as_completed(future_to_pdf):
                result = future.result()
                if result['success']:
                    success_count += 1
                else:
                    error_count += 1

                # é€Ÿç‡æ§åˆ¶ï¼šæ¯å€‹ä»»å‹™å®Œæˆå¾Œç¨ä½œå»¶é²
                time.sleep(DELAY_BETWEEN_UPLOADS / MAX_WORKERS)

    else:
        # åºåˆ—ä¸Šå‚³æ¨¡å¼ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
        for idx, pdf in enumerate(pdfs_to_upload, 1):
            result = process_single_pdf(service, pdf, state, idx, len(pdfs_to_upload))

            if result['success']:
                success_count += 1
            else:
                error_count += 1

            # é€Ÿç‡æ§åˆ¶ï¼šç­‰å¾…é¿å…è§¸ç™¼ API é™åˆ¶ï¼ˆé™¤äº†æœ€å¾Œä¸€å€‹ï¼‰
            if idx < len(pdfs_to_upload):
                print(f"  â³ ç­‰å¾… {DELAY_BETWEEN_UPLOADS} ç§’ï¼ˆé¿å… API é€Ÿç‡é™åˆ¶ï¼‰...")
                time.sleep(DELAY_BETWEEN_UPLOADS)

    # æœ€çµ‚çµ±è¨ˆ
    print("\n" + "=" * 60)
    print("ğŸ“Š ä¸Šå‚³å®Œæˆçµ±è¨ˆ")
    print("=" * 60)
    print(f"âœ… æˆåŠŸä¸Šå‚³: {success_count} å€‹æª”æ¡ˆ")
    print(f"âŒ å¤±æ•—: {error_count} å€‹æª”æ¡ˆ")

    if state['errors']:
        print(f"\nâŒ å¤±æ•—çš„æª”æ¡ˆ:")
        for error in state['errors']:
            print(f"  - {error['folder']}/{error['file']}")

    print("\nâœ… æ¸¬è©¦å®Œæˆ")
    print(f"ğŸ“ ç‹€æ…‹å·²å„²å­˜åˆ°: {STATE_FILE}")
    print("=" * 60)


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
